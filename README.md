- Analyzed performance of an agent in continuous environments with the Deep Deterministic Policy Gradient algorithm and the Actor-Critic method to learn both Q-function and Policy
- Created two customized environments with continuous action spaces-a physics-based shower environment and a financial trading-based environment
- Implemented different reward functions and recursive state-estimation to access the agentâ€™s performance in these custom environments
- Attained lower Policy loss and Q-Loss with an enhanced cumulative reward for a total of 2000 episodes
